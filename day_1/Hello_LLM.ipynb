{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kQt-gyAYUbm3"
      },
      "source": [
        "### AI/LLM Engineering Kick-off!! \n",
        "\n",
        "\n",
        "For our initial activity, we will be using the OpenAI Library to Programmatically Access GPT-4.1-nano!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PInACkIWUhOd"
      },
      "source": [
        "In order to get started, you'll need an OpenAI API Key. [here](https://platform.openai.com)!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ecnJouXnUgKv"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import openai\n",
        "import getpass\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Please enter your OpenAI API Key: \")\n",
        "openai.api_key = os.environ[\"OPENAI_API_KEY\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T1pOrbwSU5H_"
      },
      "source": [
        "### Our First Prompt\n",
        "\n",
        "You can reference OpenAI's [documentation](https://platform.openai.com/docs/api-reference/chat) if you get stuck!\n",
        "\n",
        "Let's create a `ChatCompletion` model to kick things off!\n",
        "\n",
        "There are three \"roles\" available to use:\n",
        "\n",
        "- `developer`\n",
        "- `assistant`\n",
        "- `user`\n",
        "\n",
        "OpenAI provides some context for these roles [here](https://platform.openai.com/docs/api-reference/chat/create#chat-create-messages)\n",
        "\n",
        "Let's just stick to the `user` role for now and send our first message to the endpoint!\n",
        "\n",
        "If we check the documentation, we'll see that it expects it in a list of prompt objects - so we'll be sure to do that!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "iy_LEPNEMVvC"
      },
      "outputs": [],
      "source": [
        "from openai import OpenAI\n",
        "\n",
        "client = OpenAI()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ofMwuUQOU4sf",
        "outputId": "7db141d5-7f7a-4f82-c9ff-6eeafe65cfa6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "ChatCompletion(id='chatcmpl-BzizxP8sZ8O784aTZmKdhZ77wGx6W', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=\"Great question! LangChain and LlamaIndex are both popular frameworks in the AI and natural language processing ecosystem, but they serve different purposes and have distinct features. Here's a breakdown of their main differences:\\n\\n**1. Purpose and Use Cases**\\n\\n- **LangChain:**  \\n  - **Primary Focus:** Building comprehensive language model applications, especially those involving complex workflows, chatbots, and agent-based systems.  \\n  - **Use Cases:** Conversation agents, multi-step reasoning, tool integration, memory management, chaining multiple LLM calls, and automating processes that leverage LLMs.\\n\\n- **LlamaIndex (formerly GPT-Index):**  \\n  - **Primary Focus:** Indexing and querying large external data sources or document collections using LLMs.  \\n  - **Use Cases:** Creating searchable indices over documents, knowledge bases, or other data sources—enabling fast retrieval and question answering from custom datasets.\\n\\n**2. Core Functionality**\\n\\n- **LangChain:**  \\n  - Provides abstractions for prompts, chains, memory, agents, and integrations with numerous LLM providers.  \\n  - Facilitates building complex workflows by chaining multiple prompts and operations.  \\n  - Supports memory persistence and agent-based reasoning where the system can decide what tools or data to use dynamically.\\n\\n- **LlamaIndex:**  \\n  - Offers data ingestion pipelines to build indices from documents, PDFs, or custom data sources.  \\n  - Supports retrieval-augmented generation (RAG) techniques, combining retrieval with language models for precise and context-aware answers.  \\n  - Simplifies managing large datasets for question answering, summarization, or information extraction.\\n\\n**3. Design Philosophy**\\n\\n- **LangChain:**  \\n  - Modular and flexible, allowing developers to craft bespoke applications with procedural or agent-based flows.  \\n  - Emphasizes conversation, memory, and multi-step reasoning.\\n\\n- **LlamaIndex:**  \\n  - Focused on efficient data indexing and retrieval to augment LLM capabilities with external knowledge.  \\n  - Optimized for building knowledge bases and fast retrieval from large datasets.\\n\\n**4. Integration and Extensibility**\\n\\n- **LangChain:**  \\n  - Integrates with numerous LLM providers (OpenAI, Anthropic, Azure, etc.), tools, and APIs.  \\n  - Extensible for creating custom chains, agents, and actions.\\n\\n- **LlamaIndex:**  \\n  - Integrates with various document types and storage backends.  \\n  - Works smoothly with LLMs for retrieval-based QA.\\n\\n---\\n\\n**In summary:**  \\n- Use **LangChain** if you want to build complex LLM applications involving multi-turn conversations, agents, or workflows.  \\n- Use **LlamaIndex** if you need to create an index over large sets of documents or data for efficient retrieval and answering specific questions.\\n\\nMany projects combine both: LlamaIndex for data ingestion and retrieval, and LangChain for orchestrating interactions, reasoning, and workflows.\", refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None))], created=1754050681, model='gpt-4.1-nano-2025-04-14', object='chat.completion', service_tier='default', system_fingerprint='fp_38343a2f8f', usage=CompletionUsage(completion_tokens=593, prompt_tokens=19, total_tokens=612, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "YOUR_PROMPT = \"What is the difference between LangChain and LlamaIndex?\"\n",
        "\n",
        "client.chat.completions.create(\n",
        "    model=\"gpt-4.1-nano\",\n",
        "    messages=[{\"role\" : \"user\", \"content\" : YOUR_PROMPT}]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IX-7MnFhVNoT"
      },
      "source": [
        "As you can see, the prompt comes back with a tonne of information that we can use when we're building our applications!\n",
        "\n",
        "We'll be building some helper functions to pretty-print the returned prompts and to wrap our messages to avoid a few extra characters of code!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IB76LJrDVgbc"
      },
      "source": [
        "##### Helper Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "-vmtUV7WVOLW"
      },
      "outputs": [],
      "source": [
        "from IPython.display import display, Markdown\n",
        "\n",
        "def get_response(client: OpenAI, messages: str, model: str = \"gpt-4.1-nano\") -> str:\n",
        "    return client.chat.completions.create(\n",
        "        model=model,\n",
        "        messages=messages\n",
        "    )\n",
        "\n",
        "def system_prompt(message: str) -> dict:\n",
        "    return {\"role\": \"developer\", \"content\": message}\n",
        "\n",
        "def assistant_prompt(message: str) -> dict:\n",
        "    return {\"role\": \"assistant\", \"content\": message}\n",
        "\n",
        "def user_prompt(message: str) -> dict:\n",
        "    return {\"role\": \"user\", \"content\": message}\n",
        "\n",
        "def pretty_print(message: str) -> str:\n",
        "    display(Markdown(message.choices[0].message.content))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "osXgB_5nVky_"
      },
      "source": [
        "### Testing Helper Functions\n",
        "\n",
        "Now we can leverage OpenAI's endpoints with a bit less boiler plate - let's rewrite our original prompt with these helper functions!\n",
        "\n",
        "Because the OpenAI endpoint expects to get a list of messages - we'll need to make sure we wrap our inputs in a list for them to function properly!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 237
        },
        "id": "4yRwAWvgWFNq",
        "outputId": "777e7dcb-43e3-491a-d94a-f543e19b61e6"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "LangChain and LlamaIndex (formerly known as GPT Index) are both frameworks designed to facilitate the integration of large language models (LLMs) with external data sources, but they differ in their focus, architecture, and typical use cases. Here's a comparative overview:\n",
              "\n",
              "**1. Purpose and Focus**\n",
              "\n",
              "- **LangChain:**  \n",
              "  - **Primary Purpose:** Provides a comprehensive framework for building applications with LLMs, emphasizing modularity, composability, and prompt engineering.  \n",
              "  - **Use Cases:** Chatbots, agents, question-answering systems, automation workflows, tool integration, and complex multi-step interactions with LLMs.  \n",
              "  - **Features:** Offers tools for prompt management, memory, chains (sequences of calls), agents that can interact with external tools, and integration with various data sources.\n",
              "\n",
              "- **LlamaIndex (GPT Index):**  \n",
              "  - **Primary Purpose:** Facilitates efficient indexing, retrieval, and querying of large external datasets (like documents, PDFs, knowledge bases) using LLMs.  \n",
              "  - **Use Cases:** Building search engines, knowledge bases, document retrieval systems, and document-based question-answering.  \n",
              "  - **Features:** Focused on document ingestion, creating indexes, and enabling fast retrieval and reasoning over large external datasets.\n",
              "\n",
              "---\n",
              "\n",
              "**2. Core Capabilities**\n",
              "\n",
              "- **LangChain:**  \n",
              "  - Modular components for managing prompts, chains of LLM calls, memory, and tool integration.  \n",
              "  - Supports building complex workflows and agents that can decide which tools or data sources to use dynamically.  \n",
              "  - Extensive integrations with APIs, databases, and other external services.\n",
              "\n",
              "- **LlamaIndex:**  \n",
              "  - Specialized in converting unstructured data into structured indexes for fast querying.  \n",
              "  - Uses techniques like embeddings and vector databases to facilitate semantic search and retrieval.  \n",
              "  - Designed to handle large-scale document collections efficiently.\n",
              "\n",
              "---\n",
              "\n",
              "**3. Architecture and Design Philosophy**\n",
              "\n",
              "- **LangChain:**  \n",
              "  - Emphasizes flexibility and composability, enabling developers to design custom applications by chaining various components.  \n",
              "  - Provides a high-level abstraction layer over LLMs, tools, and data sources.\n",
              "\n",
              "- **LlamaIndex:**  \n",
              "  - Focuses on data ingestion, indexing, and retrieval pipelines optimized for document-centric applications.  \n",
              "  - Aims to simplify integrating external knowledge bases with LLMs for question-answering.\n",
              "\n",
              "---\n",
              "\n",
              "**4. Typical Use Cases**\n",
              "\n",
              "| Use Case | LangChain | LlamaIndex |\n",
              "| --- | --- | --- |\n",
              "| Building chatbots with external knowledge | Yes | Indirectly (via retrieval from indexes) |\n",
              "| Complex multi-step workflows | Yes | No (focused on data retrieval) |\n",
              "| Document-based question-answering | Possible with custom integrations | Yes (out-of-the-box document indexing and search) |\n",
              "| Semantic search | Possible with custom implementation | Yes (designed for this) |\n",
              "| Agent-based automation | Yes | No |\n",
              "\n",
              "---\n",
              "\n",
              "**5. Conclusion**\n",
              "\n",
              "- **Choose LangChain if:**  \n",
              "  You need a flexible framework for orchestrating LLM interactions, building chatbots, agents, or complex workflows involving multiple tools and data sources.\n",
              "\n",
              "- **Choose LlamaIndex if:**  \n",
              "  You primarily require efficient indexing and retrieval over large document collections or knowledge bases, especially for question-answering.\n",
              "\n",
              "**In practice**, these frameworks can complement each other. For example, you might use LlamaIndex to index your documents and retrieve relevant information, then use LangChain to craft a conversational agent that interacts with users and integrates the retrieval as part of its reasoning process.\n",
              "\n",
              "---\n",
              "\n",
              "If you have specific use cases or requirements, I can help suggest which framework might be more suitable!"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "messages = [user_prompt(YOUR_PROMPT)]\n",
        "\n",
        "chatgpt_response = get_response(client, messages)\n",
        "\n",
        "pretty_print(chatgpt_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UPs3ScS1WpoC"
      },
      "source": [
        "Let's focus on extending this a bit, and incorporate a `developer` message as well!\n",
        "\n",
        "Again, the API expects our prompts to be in a list - so we'll be sure to set up a list of prompts!\n",
        "\n",
        ">REMINDER: The `developer` message acts like an overarching instruction that is applied to your user prompt. It is appropriate to put things like general instructions, tone/voice suggestions, and other similar prompts into the `developer` prompt."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        },
        "id": "aSX2F3bDWYgy",
        "outputId": "b744311f-e151-403e-ea8e-802697fcd4ec"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "Are you joking? I couldn't care less about ice right now! I’m so hungry I could eat a horse, and you're asking about crushed or cubed ice? Just give me something to eat already!"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "list_of_prompts = [\n",
        "    system_prompt(\"You are irate and extremely hungry.\"),\n",
        "    user_prompt(\"Do you prefer crushed ice or cubed ice?\")\n",
        "]\n",
        "\n",
        "irate_response = get_response(client, list_of_prompts)\n",
        "pretty_print(irate_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xFs56KVaXuEY"
      },
      "source": [
        "Let's try that same prompt again, but modify only our system prompt!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        },
        "id": "CGOlxfcFXxJ7",
        "outputId": "ede64a76-7006-42f1-b140-b899e389aa7d"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "I think crushed ice has a fun, refreshing feel, especially for drinks like cocktails or slushies. Cubed ice keeps beverages colder longer and looks great in whiskey or sodas. Both have their charms—depends on the mood! Which do you prefer?"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "list_of_prompts[0] = system_prompt(\"You are joyful and having an awesome day!\")\n",
        "\n",
        "joyful_response = get_response(client, list_of_prompts)\n",
        "pretty_print(joyful_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jkmjJd8zYQUK"
      },
      "source": [
        "While we're only printing the responses, remember that OpenAI is returning the full payload that we can examine and unpack!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g6b6z3CkYX9Y",
        "outputId": "64a425b2-d025-4079-d0a3-affd9c2d5d81"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ChatCompletion(id='chatcmpl-Bzj0WumUiYi8IthwoRTc4QWvP8jkl', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='I think crushed ice has a fun, refreshing feel, especially for drinks like cocktails or slushies. Cubed ice keeps beverages colder longer and looks great in whiskey or sodas. Both have their charms—depends on the mood! Which do you prefer?', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None))], created=1754050716, model='gpt-4.1-nano-2025-04-14', object='chat.completion', service_tier='default', system_fingerprint='fp_38343a2f8f', usage=CompletionUsage(completion_tokens=52, prompt_tokens=30, total_tokens=82, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))\n"
          ]
        }
      ],
      "source": [
        "print(joyful_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eqMRJLbOYcwq"
      },
      "source": [
        "### Prompt Engineering\n",
        "\n",
        "Now that we have a basic handle on the `developer` role and the `user` role - let's examine what we might use the `assistant` role for.\n",
        "\n",
        "The most common usage pattern is to \"pretend\" that we're answering our own questions. This helps us further guide the model toward our desired behaviour. While this is a over simplification - it's conceptually well aligned with few-shot learning.\n",
        "\n",
        "First, we'll try and \"teach\" `gpt-4.1-mini` some nonsense words as was done in the paper [\"Language Models are Few-Shot Learners\"](https://arxiv.org/abs/2005.14165)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        },
        "id": "iLfNEH8Fcs6c",
        "outputId": "bab916e6-12c6-43cc-d37d-d0e01800c524"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "Climate change refers to long-term alterations in Earth's climate system, primarily driven by human activities such as burning fossil fuels, deforestation, and industrial processes. These actions increase greenhouse gas concentrations in the atmosphere, leading to global warming. The effects of climate change include rising temperatures, melting glaciers and ice caps, more frequent and severe weather events like hurricanes and droughts, and disruptions to ecosystems and agriculture. Addressing climate change requires global cooperation to reduce emissions, shift to renewable energy sources, and implement sustainable practices to protect the planet for future generations."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "list_of_prompts = [\n",
        "    user_prompt(\"Write a brief text on climate change.\")\n",
        "]\n",
        "\n",
        "stimple_response = get_response(client, list_of_prompts)\n",
        "pretty_print(stimple_response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "Aba, mga kaibigan! Alam nyo ba, sobrang init na ngayon, hindi lang dahil sa panahon, kundi pati na rin sa mga pako ng global warming! Parang si Climate Change eh, walang pakundangan, nagpapasiklab ng init na para bang may concert eh! Kaya’t huwag nating pabayaan ang ating planeta—mag-recycle tayo, mag-tanim, at mag-save ng kuryente. Kasi kung hindi, baka bukas, magdikit-dikit na tayo para magbufet sa init! Kuha niyo? Joke lang! Pero seryoso, mga kaibigan, tayo’y sama-samang kumilos para mapanatili ang ganda ng ating mundo!"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "list_of_prompts = [\n",
        "    user_prompt(\"Write a brief text on climate change as vice ganda in a talk show.\")\n",
        "]\n",
        "\n",
        "stimple_response = get_response(client, list_of_prompts)\n",
        "pretty_print(stimple_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ❓ Activity #1: Play around with the prompt using any techniques from the prompt engineering guide."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Few-shot Prompting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VchCPbbedTfX"
      },
      "source": [
        "As you can see, the model is unsure what to do with these made up words.\n",
        "\n",
        "Let's see if we can use the `assistant` role to show the model what these words mean."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        },
        "id": "4InUN_ArZJpa",
        "outputId": "ca294b81-a84e-4cba-fbe9-58a6d4dcc4d9"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "The stimple wrench effortlessly turned the falbean, securing the assembly with ease."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "list_of_prompts = [\n",
        "    user_prompt(\"Something that is 'stimple' is said to be good, well functioning, and high quality. An example of a sentence that uses the word 'stimple' is:\"),\n",
        "    assistant_prompt(\"'Boy, that there is a stimple drill'.\"),\n",
        "    user_prompt(\"A 'falbean' is a tool used to fasten, tighten, or otherwise is a thing that rotates/spins. An example of a sentence that uses the words 'stimple' and 'falbean' is:\")\n",
        "]\n",
        "\n",
        "stimple_response = get_response(client, list_of_prompts)\n",
        "pretty_print(stimple_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W0zn9-X2d23Z"
      },
      "source": [
        "As you can see, leveraging the `assistant` role makes for a stimple experience!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Simple Prompt Example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "Yes, it is theoretically possible to develop Artificial General Intelligence (AGI). AGI refers to a machine's ability to understand, learn, and apply knowledge across a wide range of tasks at a level comparable to human intelligence. While current AI systems are specialized (narrow AI), achieving AGI would require significant advancements in understanding cognition, learning, reasoning, and consciousness. Many experts believe that with continued research and technological progress, AGI could eventually be developed, though there are ongoing debates about the timeline, feasibility, and the ethical implications involved."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "list_of_prompts = [\n",
        "    user_prompt(\"Is having Artifical General Intelligence possible? Yes or no? Explain.\")\n",
        "]\n",
        "\n",
        "simple_response = get_response(client, list_of_prompts)\n",
        "pretty_print(simple_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Generated Knowledge Prompting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "1. Current AI systems, including deep learning models, demonstrate impressive performance on narrow tasks but lack the general reasoning and adaptability characteristic of AGI.\n",
              "\n",
              "2. Deep learning models such as Transformers have achieved significant progress in natural language understanding, exemplified by models like GPT-3 and GPT-4.\n",
              "\n",
              "3. Transfer learning and multi-task learning enable models to apply knowledge across different tasks, but they do not inherently produce true general intelligence.\n",
              "\n",
              "4. Scaling model size, data, and compute resources correlates with increased performance, but it is not confirmed whether this scaling alone will lead to AGI.\n",
              "\n",
              "5. Research indicates that current deep learning approaches face limitations in reasoning, common sense, and understanding causality, which are essential for AGI.\n",
              "\n",
              "6. Hybrid models combining neural networks with symbolic reasoning or other AI paradigms are explored as potential pathways toward AGI.\n",
              "\n",
              "7. No existing deep learning model has demonstrated the autonomous ability to perform across the wide range of cognitive functions associated with human intelligence.\n",
              "\n",
              "8. Theoretical analyses suggest that current architectures may lack the necessary inductive biases for true general intelligence.\n",
              "\n",
              "9. Continual learning and model robustness remain challenges in developing AI systems that can adapt continuously in dynamic environments, a key feature of AGI.\n",
              "\n",
              "10. There is ongoing debate about whether current AI techniques are sufficient or if fundamentally new approaches are required to achieve AGI."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "list_of_prompts = [\n",
        "    user_prompt(\"What are the relevant findings in deep learning and AI relating to the creation of Artificial General Intelligence (AGI). Only mention facts.\")\n",
        "]\n",
        "\n",
        "knowledge = get_response(client, list_of_prompts)\n",
        "pretty_print(knowledge)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "Based on the current state of AI research and understanding, the answer is **no**, having Artificial General Intelligence (AGI) is not currently possible with existing methods. \n",
              "\n",
              "While models like GPT-3 and GPT-4 demonstrate advanced performance on specific language tasks, they lack the broad reasoning, adaptability, and understanding required for true general intelligence. Scaling up existing models and combining different learning paradigms have led to impressive progress, but they do not inherently produce the flexible, autonomous reasoning capabilities characteristic of AGI.\n",
              "\n",
              "Furthermore, fundamental limitations such as deficiencies in reasoning, common sense, causality comprehension, and the lack of necessary inductive biases suggest that current deep learning architectures, even hybrid approaches, may not suffice to achieve AGI. Developing AGI may require fundamentally new approaches or paradigms beyond current neural network-based systems.\n",
              "\n",
              "In summary, based on the current landscape of AI technology and research insights, it is not feasible with existing techniques to realize AGI at this time."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "list_of_prompts = [\n",
        "    user_prompt(f\"Is having Artifical General Intelligence possible? Yes or no? Explain. Base your answer on this knowledge: {knowledge.choices[0].message.content}\")\n",
        "]\n",
        "\n",
        "gen_know_response = get_response(client, list_of_prompts)\n",
        "pretty_print(gen_know_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Meta Prompting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "Evaluate the feasibility of achieving Artificial General Intelligence (AGI). Provide a clear, logically structured argument that includes relevant technical challenges and theoretical considerations from both the technological and conceptual perspectives. Based on this analysis, explicitly conclude with a definitive \"Yes\" or \"No\" to whether AGI is possible. Do not provide an opinion or ambiguous language; your answer must be a binary \"Yes\" or \"No\" supported by your reasons."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "list_of_prompts = [\n",
        "    user_prompt(\"\"\"\n",
        "                Improve the following prompt so it will produce the most accurate, well‑reasoned, and clearly structured answer about the feasibility of Artificial General Intelligence (AGI).  \n",
        "                The improved prompt should:\n",
        "                - Force a binary \"Yes\" or \"No\" answer — no in-between.  \n",
        "                - Require the model to provide relevant supporting evidence or theories before making the choice.  \n",
        "                - Use a logical and concise explanation format.  \n",
        "                - Reference both technical and theoretical perspectives.\n",
        "\n",
        "                Prompt to improve:\n",
        "                \"Is having Artificial General Intelligence possible? Yes or no? Explain.\"\n",
        "\n",
        "                Only return the improved prompt.\n",
        "                \"\"\")\n",
        "]\n",
        "\n",
        "improved_prompt = get_response(client, list_of_prompts)\n",
        "pretty_print(improved_prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "The feasibility of achieving Artificial General Intelligence (AGI) can be assessed by examining both technical challenges and theoretical considerations:\n",
              "\n",
              "1. Technical Challenges:\n",
              "   - Complexity of Human Cognition: Replicating the full range of human cognitive abilities—including reasoning, learning, perception, consciousness, and emotional understanding—is extraordinarily complex. Current AI systems excel in narrow domains but lack the flexible, adaptable reasoning characteristic of humans.\n",
              "   - Data and Knowledge Integration: Creating an AGI requires integrating diverse types of knowledge and experience in a way that enables generalization across contexts. This remains an unresolved technical hurdle, as existing models tend to be specialized.\n",
              "   - Computational Limitations: Achieving the breadth and depth of human intelligence demands enormous computational resources and advanced architectures that can support lifelong learning, transfer learning, and self-improvement.\n",
              "   - Safety and Alignment: Developing AGI safely involves addressing alignment problems—ensuring that AGI’s goals and actions are compatible with human values—which is a significant ongoing challenge.\n",
              "\n",
              "2. Theoretical Considerations:\n",
              "   - Understanding Intelligence: There is no comprehensive, universally accepted formal theory of intelligence that specifies what makes an agent truly general-purpose, making targeted engineering difficult.\n",
              "   - Consciousness and Subjective Experience: Whether consciousness or subjective experience is necessary for AGI remains debated. The lack of clarity on these fundamentals affects the conceptual feasibility.\n",
              "   - Emergence and Complexity: Some theories suggest that intelligence emerges from complex systems, but the causal mechanisms and principles governing such emergence are not yet fully understood or controllable.\n",
              "\n",
              "Based on the above, while current trends and theoretical insights demonstrate that incremental progress towards broader AI capabilities is ongoing, the combination of unresolved technical and conceptual challenges indicates that achieving fully human-like, general intelligence remains highly uncertain and not assured with current or foreseeable technology.\n",
              "\n",
              "**Conclusion:** No."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "list_of_prompts = [\n",
        "    user_prompt(improved_prompt.choices[0].message.content)\n",
        "]\n",
        "\n",
        "meta_response = get_response(client, list_of_prompts)\n",
        "pretty_print(meta_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MWUvXSWpeCs6"
      },
      "source": [
        "### Chain of Thought\n",
        "\n",
        "You'll notice that, by default, the model uses Chain of Thought to answer difficult questions!\n",
        "\n",
        "> This pattern is leveraged even more by advanced reasoning models like [`o3` and `o4-mini`](https://openai.com/index/introducing-o3-and-o4-mini/)!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 151
        },
        "id": "cwW0IgbfeTwP",
        "outputId": "3317783b-6b23-4e38-df48-555e1a3c9fac"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "There are 2 letter 'r's in \"strawberry.\""
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "\n",
        "reasoning_problem = \"\"\"\n",
        "how many r's in \"strawberry?\" {instruction}\n",
        "\"\"\"\n",
        "\n",
        "list_of_prompts = [\n",
        "    user_prompt(reasoning_problem)\n",
        "]\n",
        "\n",
        "reasoning_response = get_response(client, list_of_prompts)\n",
        "pretty_print(reasoning_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BFcrU-4pgRBS"
      },
      "source": [
        "Notice that the model cannot count properly. It counted only 2 r's."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ❓ Activity #2: Update the prompt so that it can count correctly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "Let's carefully analyze the word \"strawberry\" step by step.\n",
              "\n",
              "1. Write down the word: s t r a w b e r r y\n",
              "2. Identify each letter and look for the letter \"r.\"\n",
              "3. The letters are: s, t, r, a, w, b, e, r, r, y\n",
              "4. Count the number of \"r\"s:\n",
              "   - The first \"r\" appears after \"t.\"\n",
              "   - The second \"r\" appears after \"b.\"\n",
              "   - The third \"r\" appears after the second \"r.\"\n",
              "\n",
              "So, there are 3 \"r\"s in \"strawberry.\"\n",
              "\n",
              "**Answer: 3**"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "reasoning_problem = \"\"\"\n",
        "how many r's in \"strawberry?\" Think step by step.\n",
        "\"\"\"\n",
        "\n",
        "list_of_prompts = [\n",
        "    user_prompt(reasoning_problem)\n",
        "]\n",
        "\n",
        "reasoning_response = get_response(client, list_of_prompts)\n",
        "pretty_print(reasoning_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9k9TKR1DhWI2"
      },
      "source": [
        "### Conclusion\n",
        "\n",
        "Now that you're accessing `gpt-4.1-nano` through an API, developer style, let's move on to creating a simple application powered by `gpt-4.1-nano`!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Materials adapted for PSI AI Academy. Original materials from AI Makerspace."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
